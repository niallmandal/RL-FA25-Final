{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-asset PPO trading setup\n",
    "\n",
    "Assumptions:\n",
    "- prices: np.ndarray of shape (T, N_assets)\n",
    "- features: np.ndarray of shape (T, F), already aligned with prices by time\n",
    "- At each step t:\n",
    "    * Agent observes state s_t = [features_t, position_pct, cash_pct]\n",
    "    * Agent outputs a_t in [-1,1]^N_assets (one PPO output per asset)\n",
    "    * We follow your mapping:\n",
    "        E_t = cash_t + sum(p_i * S_i_t)\n",
    "        For N assets, each asset gets E_t / N as base scale:\n",
    "            V_i = (E_t / N) * a_i\n",
    "            N_i_target = V_i / S_i_t\n",
    "            Delta_i = N_i_target - p_i (trade size)\n",
    "    * Reward is portfolio return from t to t+1 minus transaction costs\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 1. Trading Environment (gymnasium style)\n",
    "# ==========================\n",
    "\n",
    "class MultiAssetTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    FinRL-style multi-asset trading environment with continuous actions,\n",
    "    following your order-size mapping diagram.\n",
    "\n",
    "    State: [features_t, position_value_pct, cash_pct]\n",
    "    Action: Box[-1,1] of size N_assets\n",
    "    Reward: portfolio return from t to t+1 (after trades) minus transaction cost\n",
    "\n",
    "    Gymnasium API:\n",
    "        reset() -> obs, info\n",
    "        step(action) -> obs, reward, terminated, truncated, info\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices: np.ndarray,\n",
    "        features: np.ndarray,\n",
    "        initial_cash: float = 100000.0,\n",
    "        transaction_cost: float = 0.0005,  # 5 bps per dollar traded\n",
    "        render_mode: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert prices.shape[0] == features.shape[0], \\\n",
    "            \"prices and features must have the same time dimension\"\n",
    "\n",
    "        self.prices = prices.astype(np.float32)\n",
    "        self.features = features.astype(np.float32)\n",
    "\n",
    "        self.T, self.n_assets = self.prices.shape\n",
    "        self.feature_dim = self.features.shape[1]\n",
    "\n",
    "        self.initial_cash = float(initial_cash)\n",
    "        self.transaction_cost = float(transaction_cost)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Action: continuous allocation signals in [-1,1] for each asset\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self.n_assets,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Observation: features + position percentage per asset + cash percentage\n",
    "        self.obs_dim = self.feature_dim + self.n_assets + 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.obs_dim,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Internal state\n",
    "        self._reset_internal_state()\n",
    "\n",
    "    # ---------- internal helpers ----------\n",
    "\n",
    "    def _reset_internal_state(self):\n",
    "        self.t = 0  # current time index\n",
    "        self.positions = np.zeros(self.n_assets, dtype=np.float32)  # shares\n",
    "        self.cash = float(self.initial_cash)\n",
    "        self.equity = float(self.initial_cash)\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build observation vector at current time index self.t:\n",
    "        [features_t, position_value_pct, cash_pct]\n",
    "        \"\"\"\n",
    "        features_t = self.features[self.t]  # shape (F,)\n",
    "        price_t = self.prices[self.t]       # shape (N_assets,)\n",
    "\n",
    "        position_values = self.positions * price_t  # dollar value per asset\n",
    "        total_equity = self.equity + 1e-8\n",
    "\n",
    "        position_pct = position_values / total_equity  # shape (N_assets,)\n",
    "        cash_pct = np.array([self.cash / total_equity], dtype=np.float32)\n",
    "\n",
    "        obs = np.concatenate(\n",
    "            [features_t, position_pct.astype(np.float32), cash_pct],\n",
    "            axis=0\n",
    "        )\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    # ---------- gymnasium API ----------\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        \"\"\"\n",
    "        Gymnasium reset: returns (obs, info)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self._reset_internal_state()\n",
    "        obs = self._get_observation()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One trading step:\n",
    "        - Use current prices S_t and action a_t to compute desired target positions,\n",
    "          following your mapping:\n",
    "            E_t = cash + sum(p_i * S_i_t)\n",
    "            For general N assets:\n",
    "                base_scale = E_t / N\n",
    "                V_i = base_scale * a_i\n",
    "                N_i_target = V_i / S_i_t\n",
    "                Delta_i = N_i_target - p_i\n",
    "        - Apply trades, charge transaction cost on traded notional\n",
    "        - Move to next time t+1, compute portfolio return as reward\n",
    "        \"\"\"\n",
    "        # Clip action into [-1,1]\n",
    "        action = np.clip(action, -1.0, 1.0).astype(np.float32)\n",
    "        price_t = self.prices[self.t]  # prices at time t\n",
    "        price_tp1 = self.prices[self.t + 1] if self.t + 1 < self.T else price_t\n",
    "\n",
    "        # 1. Compute current equity before trades\n",
    "        current_position_value = np.sum(self.positions * price_t)\n",
    "        self.equity = self.cash + current_position_value\n",
    "\n",
    "        # 2. Mapping: compute target dollar exposure and shares\n",
    "        base_scale = self.equity / float(self.n_assets)  # E / N\n",
    "        target_values = base_scale * action  # V_i = E/N * a_i\n",
    "\n",
    "        # Convert target dollar values to target positions (in shares)\n",
    "        target_positions = np.where(\n",
    "            price_t > 0,\n",
    "            target_values / price_t,\n",
    "            0.0,\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Trade size (Delta_i)\n",
    "        trade_shares = target_positions - self.positions  # + buy, - sell\n",
    "\n",
    "        # 3. Apply trades, charge transaction cost\n",
    "        trade_values = trade_shares * price_t  # signed trade notional per asset\n",
    "        # total dollar turnover (absolute)\n",
    "        dollar_turnover = np.sum(np.abs(trade_values))\n",
    "        transaction_costs = self.transaction_cost * dollar_turnover\n",
    "\n",
    "        # Update cash and positions:\n",
    "        self.cash = self.cash - np.sum(trade_values) - transaction_costs\n",
    "        self.positions = self.positions + trade_shares\n",
    "\n",
    "        # 4. Move to t+1 and compute new portfolio value\n",
    "        new_position_value = np.sum(self.positions * price_tp1)\n",
    "        new_equity = self.cash + new_position_value\n",
    "\n",
    "        # Reward: simple portfolio return from t to t+1\n",
    "        reward = (new_equity - self.equity) / (self.equity + 1e-8)\n",
    "\n",
    "        # Update equity and time\n",
    "        self.equity = new_equity\n",
    "        self.t += 1\n",
    "\n",
    "        # Episode termination: reached last usable time index\n",
    "        terminated = self.t >= self.T - 1\n",
    "        truncated = False  # 可以加 max_steps 等逻辑，这里先不截断\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        info = {\n",
    "            \"equity\": self.equity,\n",
    "            \"cash\": self.cash,\n",
    "            \"positions\": self.positions.copy(),\n",
    "        }\n",
    "\n",
    "        # Gymnasium: (obs, reward, terminated, truncated, info)\n",
    "        return obs, float(reward), terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"t={self.t}, equity={self.equity:.2f}, cash={self.cash:.2f}, \"\n",
    "            f\"positions={self.positions}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 2. PPO Agent\n",
    "# ==========================\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared base network with separate policy (mean) and value heads.\n",
    "    Output distribution is Normal; actions are later clipped into [-1,1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, action_dim, hidden_sizes=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        self.base = nn.Sequential(*layers)\n",
    "\n",
    "        self.mu_head = nn.Linear(input_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        self.value_head = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base(x)\n",
    "        # keep mean in [-1,1] via tanh; we still use Normal and clip later\n",
    "        mu = torch.tanh(self.mu_head(base))\n",
    "        std = torch.exp(self.log_std)\n",
    "        value = self.value_head(base).squeeze(-1)\n",
    "        return mu, std, value\n",
    "\n",
    "    def get_dist_and_value(self, obs):\n",
    "        mu, std, value = self.forward(obs)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim,\n",
    "        action_dim,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        vf_coef=0.5,\n",
    "        ent_coef=0.0,\n",
    "        max_grad_norm=0.5,\n",
    "        device=None,\n",
    "    ):\n",
    "        self.device = device or (\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.net = PolicyValueNet(obs_dim, action_dim).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Given a single observation (numpy array), return action (numpy),\n",
    "        log_prob, and value estimate.\n",
    "        \"\"\"\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        dist, value = self.net.get_dist_and_value(obs_t)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "        action_np = action.squeeze(0).cpu().numpy()\n",
    "        log_prob_np = log_prob.item()\n",
    "        value_np = value.item()\n",
    "        return np.clip(action_np, -1.0, 1.0), log_prob_np, value_np\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones, last_value):\n",
    "        \"\"\"\n",
    "        Generalized Advantage Estimation (GAE-Lambda)\n",
    "        rewards, values, dones: numpy arrays length T\n",
    "        last_value: scalar, V(s_{T})\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        adv = np.zeros(T, dtype=np.float32)\n",
    "        last_adv = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            mask = 1.0 - float(dones[t])\n",
    "            delta = rewards[t] + self.gamma * last_value * mask - values[t]\n",
    "            last_adv = delta + self.gamma * self.lam * mask * last_adv\n",
    "            adv[t] = last_adv\n",
    "            last_value = values[t]\n",
    "        returns = values + adv\n",
    "        return adv, returns\n",
    "\n",
    "    def update(self, batch, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        batch: dict with keys\n",
    "            'obs', 'actions', 'log_probs', 'returns', 'advantages'\n",
    "        \"\"\"\n",
    "        obs = torch.as_tensor(batch[\"obs\"], dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(batch[\"actions\"], dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.as_tensor(batch[\"log_probs\"], dtype=torch.float32, device=self.device)\n",
    "        returns = torch.as_tensor(batch[\"returns\"], dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.as_tensor(batch[\"advantages\"], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        n = obs.size(0)\n",
    "        idxs = np.arange(n)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, n, batch_size):\n",
    "                end = start + batch_size\n",
    "                mb_idx = idxs[start:end]\n",
    "\n",
    "                mb_obs = obs[mb_idx]\n",
    "                mb_actions = actions[mb_idx]\n",
    "                mb_old_log_probs = old_log_probs[mb_idx]\n",
    "                mb_returns = returns[mb_idx]\n",
    "                mb_advantages = advantages[mb_idx]\n",
    "\n",
    "                dist, values = self.net.get_dist_and_value(mb_obs)\n",
    "                new_log_probs = dist.log_prob(mb_actions).sum(-1)\n",
    "                entropy = dist.entropy().sum(-1).mean()\n",
    "\n",
    "                # Policy loss (clipped surrogate objective)\n",
    "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(\n",
    "                    ratio,\n",
    "                    1.0 - self.clip_ratio,\n",
    "                    1.0 + self.clip_ratio\n",
    "                ) * mb_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = nn.functional.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 3. Minimal training loop (compatible with gymnasium)\n",
    "# ==========================\n",
    "\n",
    "def train_ppo_on_env(\n",
    "    env: MultiAssetTradingEnv,\n",
    "    agent: PPOAgent,\n",
    "    total_steps: int = 50_000,\n",
    "    rollout_horizon: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal PPO training loop.\n",
    "    You can adapt this to log training curves, save models, etc.\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    step_count = 0\n",
    "\n",
    "    while step_count < total_steps:\n",
    "        # Collect one rollout\n",
    "        obs_buf = []\n",
    "        act_buf = []\n",
    "        logp_buf = []\n",
    "        rew_buf = []\n",
    "        val_buf = []\n",
    "        done_buf = []\n",
    "\n",
    "        for _ in range(rollout_horizon):\n",
    "            action, logp, value = agent.act(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            obs_buf.append(obs)\n",
    "            act_buf.append(action)\n",
    "            logp_buf.append(logp)\n",
    "            rew_buf.append(reward)\n",
    "            val_buf.append(value)\n",
    "            done_buf.append(done)\n",
    "\n",
    "            obs = next_obs\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                obs, info = env.reset()\n",
    "\n",
    "            if step_count >= total_steps:\n",
    "                break\n",
    "\n",
    "        # Compute advantage & returns using last value estimate\n",
    "        with torch.no_grad():\n",
    "            last_value = agent.net.get_dist_and_value(\n",
    "                torch.as_tensor(\n",
    "                    obs, dtype=torch.float32, device=agent.device\n",
    "                ).unsqueeze(0)\n",
    "            )[1].item()\n",
    "\n",
    "        obs_arr = np.array(obs_buf, dtype=np.float32)\n",
    "        act_arr = np.array(act_buf, dtype=np.float32)\n",
    "        logp_arr = np.array(logp_buf, dtype=np.float32)\n",
    "        rew_arr = np.array(rew_buf, dtype=np.float32)\n",
    "        val_arr = np.array(val_buf, dtype=np.float32)\n",
    "        done_arr = np.array(done_buf, dtype=bool)\n",
    "\n",
    "        adv, ret = agent.compute_gae(rew_arr, val_arr, done_arr, last_value)\n",
    "\n",
    "        batch = {\n",
    "            \"obs\": obs_arr,\n",
    "            \"actions\": act_arr,\n",
    "            \"log_probs\": logp_arr,\n",
    "            \"returns\": ret,\n",
    "            \"advantages\": adv,\n",
    "        }\n",
    "\n",
    "        agent.update(batch)\n",
    "\n",
    "        print(f\"Trained up to step {step_count}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2ad0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained up to step 256\n",
      "Trained up to step 512\n",
      "Trained up to step 768\n",
      "Trained up to step 1024\n",
      "Trained up to step 1280\n",
      "Trained up to step 1536\n",
      "Trained up to step 1792\n",
      "Trained up to step 2048\n",
      "Trained up to step 2304\n",
      "Trained up to step 2560\n",
      "Trained up to step 2816\n",
      "Trained up to step 3072\n",
      "Trained up to step 3328\n",
      "Trained up to step 3584\n",
      "Trained up to step 3840\n",
      "Trained up to step 4096\n",
      "Trained up to step 4352\n",
      "Trained up to step 4608\n",
      "Trained up to step 4864\n",
      "Trained up to step 5000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Example usage with dummy data\n",
    "# ==========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: 5 ETFs, 1000 days, 20 signals per day\n",
    "    T = 1000\n",
    "    N_assets = 5\n",
    "    F = 20\n",
    "\n",
    "    # Dummy price paths (random walk)\n",
    "    rng = np.random.default_rng(42)\n",
    "    log_returns = rng.normal(0, 0.01, size=(T, N_assets))\n",
    "    prices = 100 * np.exp(np.cumsum(log_returns, axis=0)).astype(np.float32)\n",
    "\n",
    "    # Dummy features (here just some random numbers; in your project use real signals)\n",
    "    features = rng.normal(size=(T, F)).astype(np.float32)\n",
    "\n",
    "    env = MultiAssetTradingEnv(\n",
    "        prices=prices,\n",
    "        features=features,\n",
    "        initial_cash=100000.0,\n",
    "        transaction_cost=0.0005,\n",
    "    )\n",
    "    ppo_agent = PPOAgent(\n",
    "        obs_dim=env.obs_dim,\n",
    "        action_dim=env.n_assets,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_ratio=0.2,\n",
    "    )\n",
    "\n",
    "    train_ppo_on_env(env, ppo_agent, total_steps=5000, rollout_horizon=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e753cd",
   "metadata": {},
   "source": [
    "### Update the RL framework with new data and signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ada0b",
   "metadata": {},
   "source": [
    "\n",
    "Multi-asset PPO trading setup using team-provided minute data and signals.\n",
    "\n",
    "Data sources (all already pushed by your teammates):\n",
    "\n",
    "- `data.csv`  \n",
    "  DateTime index at 1-minute frequency.  \n",
    "  Columns like: `VOO_Close`, `VOO_Open`, `VOO_High`, `VOO_Low`, `VOO_Volume`, ...  \n",
    "  for tickers: VOO, IEMG, GLDM, TLT, HYG.\n",
    "\n",
    "- `volatility_forecasts.csv`  \n",
    "  DateTime index (with timezone), columns: VOO, IEMG, GLDM, TLT, HYG.  \n",
    "  Each is a 1-step ahead GARCH volatility forecast.\n",
    "\n",
    "- `A6.csv`  \n",
    "  DateTime index (with timezone), columns: VOO, IEMG, GLDM, TLT, HYG.  \n",
    "  Alpha #6:\n",
    "  \n",
    "  $$\n",
    "  \\alpha^{(6)}_t = -\\mathrm{corr}_{10}(\\text{Open}, \\text{Volume})\n",
    "  $$\n",
    "\n",
    "- `A101.csv`  \n",
    "  DateTime index (with timezone), columns: VOO, IEMG, GLDM, TLT, HYG.  \n",
    "  Alpha #101:\n",
    "  \n",
    "  $$\n",
    "  \\alpha^{(101)}_t =\n",
    "  \\frac{\\text{Close}_t - \\text{Open}_t}\n",
    "       {(\\text{High}_t - \\text{Low}_t) + 0.001}\n",
    "  $$\n",
    "\n",
    "- `factor_outputs/`  \n",
    "  Many files like `OHLC-open_lag_1_clean.csv`, `OHLC-high_lag_3_clean.csv`, ...  \n",
    "  Each has columns: `[DateTime, ticker, <factor_name>]`.  \n",
    "  These factors have already been shifted by 1 bar in `OHLC.py` to avoid look-ahead.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions in this RL setup\n",
    "\n",
    "**State at time \\(t\\)**  \n",
    "\n",
    "$$\n",
    "s_t = \\big[\\text{all signals at time } t,\\ \\text{position value percentages},\\ \\text{cash percentage}\\big].\n",
    "$$\n",
    "\n",
    "**Action at time \\(t\\)**  \n",
    "\n",
    "$$\n",
    "a_t \\in [-1, 1]^{N_{\\text{assets}}}\n",
    "$$\n",
    "\n",
    "(one continuous signal per asset).\n",
    "\n",
    "---\n",
    "\n",
    "### Mapping from action to target position\n",
    "\n",
    "Portfolio equity:\n",
    "\n",
    "$$\n",
    "E_t = \\text{cash}_t + \\sum_i p_i\\, S_{i,t}\n",
    "$$\n",
    "\n",
    "where \\(p_i\\) is the number of units of asset \\(i\\) and \\(S_{i,t}\\) is its price at time \\(t\\).\n",
    "\n",
    "Base scale per asset:\n",
    "\n",
    "$$\n",
    "\\text{base\\_scale} = \\frac{E_t}{N_{\\text{assets}}}\n",
    "$$\n",
    "\n",
    "Dollar value assigned to asset \\(i\\):\n",
    "\n",
    "$$\n",
    "V_i = \\text{base\\_scale} \\cdot a_i\n",
    "$$\n",
    "\n",
    "Target number of units for asset \\(i\\):\n",
    "\n",
    "$$\n",
    "N^{\\text{target}}_{i,t} = \\frac{V_i}{S_{i,t}}\n",
    "$$\n",
    "\n",
    "Trade size (change in units):\n",
    "\n",
    "$$\n",
    "\\Delta_i = N^{\\text{target}}_{i,t} - p_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Reward\n",
    "\n",
    "Portfolio return from \\(t\\) to \\(t+1\\), net of transaction costs, based on minute-to-minute price changes:\n",
    "\n",
    "$$\n",
    "r_{t+1} \n",
    "= \\frac{E_{t+1} - E_t}{E_t}\n",
    "\\;-\\; \\text{transaction\\_costs}_{t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c142acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_price_and_features(\n",
    "    data_path: str = \"data.csv\",\n",
    "    vol_path: str = \"volatility_forecasts.csv\",\n",
    "    a6_path: str = \"A6.csv\",\n",
    "    a101_path: str = \"A101.csv\",\n",
    "    factor_dir: str = \"factor_outputs\",\n",
    "    tickers=(\"VOO\", \"IEMG\", \"GLDM\", \"TLT\", \"HYG\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Build the price matrix (for reward calculation) and the feature matrix (state signals)\n",
    "    from the team's minute-level data and signal CSVs.\n",
    "\n",
    "    This function:\n",
    "    - Reads prices from data.csv (OHLCV).\n",
    "    - Reads vol / A6 / A101 from their CSVs.\n",
    "    - Reads OHLC factors from factor_outputs/*_clean.csv.\n",
    "    - Normalizes ALL timestamps to a common UTC-based naive timeline.\n",
    "    - Aligns all signals to the same time index.\n",
    "    - Handles NaN/Inf and drops warm-up periods.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Helpers: normalize timestamps via UTC ----------\n",
    "    def _normalize_index_utc_for_index(x):\n",
    "        \"\"\"\n",
    "        For Index-like objects (e.g. df.index or an array of DateTime strings):\n",
    "        parse as UTC, then drop timezone and return a DatetimeIndex.\n",
    "\n",
    "        Examples:\n",
    "        - '2025-08-04 09:30:00'        -> 09:30 UTC -> 09:30 (naive)\n",
    "        - '2025-08-04 05:30:00-04:00'  -> 09:30 UTC -> 09:30 (naive)\n",
    "        \"\"\"\n",
    "        dt = pd.to_datetime(x, utc=True)\n",
    "        if not isinstance(dt, pd.DatetimeIndex):\n",
    "            dt = pd.DatetimeIndex(dt)\n",
    "        return dt.tz_convert(None)\n",
    "\n",
    "    def _normalize_index_utc_for_series(s: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        For a pandas Series column (like base_df['DateTime']):\n",
    "        parse as UTC, then drop timezone and return a Series of naive datetimes.\n",
    "        \"\"\"\n",
    "        dt = pd.to_datetime(s, utc=True)\n",
    "        return dt.dt.tz_convert(None)\n",
    "\n",
    "    # ---------- Helper for reading signal CSVs (vol / A6 / A101) ----------\n",
    "    def _read_signal_csv(path, tickers, ref_idx, shift_one=False, prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Read a signal CSV whose index is DateTime (string), normalize time,\n",
    "        align to ref_idx, and optionally lag by one bar (to avoid look-ahead).\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(path):\n",
    "            raise FileNotFoundError(f\"Signal file not found: {path}\")\n",
    "\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "        # Normalize index to the same UTC-based naive timeline as data.csv\n",
    "        df.index = _normalize_index_utc_for_index(df.index)\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # Keep only requested tickers that actually exist in the file\n",
    "        available_cols = [tic for tic in tickers if tic in df.columns]\n",
    "        missing_cols = [tic for tic in tickers if tic not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"[WARN] {os.path.basename(path)} is missing tickers: {missing_cols}\")\n",
    "        if not available_cols:\n",
    "            print(f\"[WARN] {os.path.basename(path)} has no usable ticker columns; returning empty frame.\")\n",
    "            return pd.DataFrame(index=ref_idx)\n",
    "\n",
    "        df = df[available_cols]\n",
    "\n",
    "        # Lag by 1 bar if requested (A6 / A101)\n",
    "        if shift_one:\n",
    "            df = df.shift(1)\n",
    "\n",
    "        # Align to reference index from data.csv\n",
    "        df = df.reindex(ref_idx)\n",
    "\n",
    "        # Rename columns with prefix: vol_VOO, a6_VOO, a101_VOO, ...\n",
    "        df.columns = [f\"{prefix}_{tic}\" for tic in available_cols]\n",
    "        return df\n",
    "\n",
    "    # ---------- 1. Base price data: data.csv ----------\n",
    "    base_df = pd.read_csv(data_path)\n",
    "    # Normalize DateTime column using the same UTC-based logic\n",
    "    base_df[\"DateTime\"] = _normalize_index_utc_for_series(base_df[\"DateTime\"])\n",
    "    base_df = base_df.sort_values(\"DateTime\").set_index(\"DateTime\")\n",
    "\n",
    "    # Use Close prices as tradable prices\n",
    "    price_cols = [f\"{tic}_Close\" for tic in tickers]\n",
    "    for c in price_cols:\n",
    "        if c not in base_df.columns:\n",
    "            raise ValueError(f\"Column {c} not found in {data_path}\")\n",
    "    prices_df = base_df[price_cols].copy()\n",
    "    prices_df.columns = [f\"close_{tic}\" for tic in tickers]\n",
    "\n",
    "    # Reference time index\n",
    "    idx = prices_df.index.copy()\n",
    "\n",
    "    # ---------- 2. Volatility forecasts ----------\n",
    "    vol_df = _read_signal_csv(\n",
    "        path=vol_path,\n",
    "        tickers=tickers,\n",
    "        ref_idx=idx,\n",
    "        shift_one=False,\n",
    "        prefix=\"vol\",\n",
    "    )\n",
    "\n",
    "    # ---------- 3. Alpha 6 ----------\n",
    "    a6_df = _read_signal_csv(\n",
    "        path=a6_path,\n",
    "        tickers=tickers,\n",
    "        ref_idx=idx,\n",
    "        shift_one=True,   # use A6(t-1) at time t\n",
    "        prefix=\"a6\",\n",
    "    )\n",
    "\n",
    "    # ---------- 4. Alpha 101 ----------\n",
    "    a101_df = _read_signal_csv(\n",
    "        path=a101_path,\n",
    "        tickers=tickers,\n",
    "        ref_idx=idx,\n",
    "        shift_one=True,   # use A101(t-1) at time t\n",
    "        prefix=\"a101\",\n",
    "    )\n",
    "\n",
    "    # ---------- 5. OHLC factor files: factor_outputs/*_clean.csv ----------\n",
    "    # Each *_clean.csv has columns: DateTime, ticker, <factor_name>.\n",
    "    # These are already shifted by 1 (no look-ahead) and dropna'ed in your OHLC script.\n",
    "    factor_frames = []\n",
    "    if os.path.isdir(factor_dir):\n",
    "        clean_files = glob.glob(os.path.join(factor_dir, \"*_clean.csv\"))\n",
    "        for path in clean_files:\n",
    "            fname = os.path.basename(path)\n",
    "            if not fname.endswith(\"_clean.csv\"):\n",
    "                continue\n",
    "            factor_name = fname.replace(\"_clean.csv\", \"\")\n",
    "\n",
    "            df_factor = pd.read_csv(path)\n",
    "            if \"DateTime\" not in df_factor.columns or \"ticker\" not in df_factor.columns:\n",
    "                continue\n",
    "\n",
    "            # Normalize DateTime exactly the same way as data.csv\n",
    "            df_factor[\"DateTime\"] = _normalize_index_utc_for_series(df_factor[\"DateTime\"])\n",
    "            df_factor = df_factor.sort_values([\"DateTime\", \"ticker\"])\n",
    "\n",
    "            # Identify the factor value column\n",
    "            value_col_candidates = [c for c in df_factor.columns if c not in [\"DateTime\", \"ticker\"]]\n",
    "            if len(value_col_candidates) != 1:\n",
    "                # Unexpected structure; skip this file\n",
    "                continue\n",
    "            value_col = value_col_candidates[0]\n",
    "\n",
    "            # Wide form: index = DateTime, columns = ticker, values = factor\n",
    "            pivot = df_factor.pivot(index=\"DateTime\", columns=\"ticker\", values=value_col)\n",
    "\n",
    "            # Align to data.csv time index\n",
    "            pivot = pivot.reindex(idx)\n",
    "\n",
    "            # Keep only requested tickers\n",
    "            pivot = pivot[[tic for tic in tickers if tic in pivot.columns]]\n",
    "            if pivot.shape[1] == 0:\n",
    "                continue\n",
    "\n",
    "            # Rename columns to factor_ticker\n",
    "            pivot.columns = [f\"{factor_name}_{tic}\" for tic in pivot.columns]\n",
    "            factor_frames.append(pivot)\n",
    "\n",
    "    # ---------- 6. Combine everything into a single feature DataFrame ----------\n",
    "    feat_df = pd.DataFrame(index=idx)\n",
    "\n",
    "    for block in [vol_df, a6_df, a101_df] + factor_frames:\n",
    "        if block is not None and not block.empty:\n",
    "            feat_df = feat_df.join(block, how=\"left\")\n",
    "\n",
    "    # Replace +/-Inf with NaN first\n",
    "    feat_df = feat_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Drop feature columns that are entirely NaN (e.g. signals with no overlap)\n",
    "    all_nan_cols = feat_df.columns[feat_df.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "        print(\"[WARN] Dropping all-NaN feature columns:\", list(all_nan_cols))\n",
    "        feat_df = feat_df.drop(columns=list(all_nan_cols))\n",
    "\n",
    "    if feat_df.shape[1] == 0:\n",
    "        raise ValueError(\n",
    "            \"No feature columns remain after combining signals. \"\n",
    "            \"Check that your volatility/A6/A101/OHLC CSVs have the expected tickers and structure.\"\n",
    "        )\n",
    "\n",
    "    # Drop any rows that still contain NaNs (warm-up / misaligned periods)\n",
    "    before_rows = len(feat_df)\n",
    "    feat_df = feat_df.dropna()\n",
    "    after_rows = len(feat_df)\n",
    "    print(f\"[INFO] Dropped {before_rows - after_rows} rows with missing or infinite features; remaining {after_rows} rows.\")\n",
    "\n",
    "    # Align prices to the same cleaned index\n",
    "    prices_df = prices_df.reindex(feat_df.index)\n",
    "\n",
    "    if prices_df.isnull().any().any():\n",
    "        raise ValueError(\n",
    "            \"NaNs remain in prices after aligning with features. \"\n",
    "            \"Check that data.csv covers the full period used by the signals.\"\n",
    "        )\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    price_mat = prices_df.to_numpy(dtype=np.float32)     # (T, N_assets)\n",
    "    feature_mat = feat_df.to_numpy(dtype=np.float32)     # (T, F)\n",
    "    feature_names = list(feat_df.columns)\n",
    "\n",
    "    # Final safety: replace any remaining non-finite values in features with 0.0\n",
    "    non_finite_mask = ~np.isfinite(feature_mat)\n",
    "    if non_finite_mask.any():\n",
    "        print(\"[WARN] Replacing remaining non-finite feature values with 0.0\")\n",
    "        feature_mat[non_finite_mask] = 0.0\n",
    "\n",
    "    if not np.isfinite(price_mat).all():\n",
    "        raise ValueError(\"Non-finite values found in price_mat after alignment.\")\n",
    "\n",
    "    print(\"Sample index from raw data:\", idx[:3])\n",
    "    print(\"Sample index after alignment:\", feat_df.index[:3])\n",
    "\n",
    "    return price_mat, feature_mat, feat_df.index, feature_names\n",
    "\n",
    "# ============================================================\n",
    "# 1. Trading Environment (gymnasium style)\n",
    "# ============================================================\n",
    "\n",
    "class MultiAssetTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Multi-asset trading environment with continuous actions, using Gymnasium API.\n",
    "\n",
    "    - State: [features_t, position_value_pct, cash_pct]\n",
    "    - Action: Box[-1,1] of size N_assets (one continuous action per asset)\n",
    "    - Reward: portfolio return from t to t+1 (after applying trades and transaction costs)\n",
    "\n",
    "    Mapping from action to position follows the design:\n",
    "        E_t = cash_t + sum_i p_i * S_i_t\n",
    "        base_scale = E_t / N_assets\n",
    "        V_i = base_scale * a_i\n",
    "        N_i_target = V_i / S_i_t\n",
    "        Delta_i = N_i_target - p_i\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices: np.ndarray,\n",
    "        features: np.ndarray,\n",
    "        initial_cash: float = 100000.0,\n",
    "        transaction_cost: float = 0.0005,  # 5 bps per dollar traded\n",
    "        render_mode: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert prices.shape[0] == features.shape[0], \\\n",
    "            \"prices and features must have the same time dimension\"\n",
    "\n",
    "        self.prices = prices.astype(np.float32)\n",
    "        self.features = features.astype(np.float32)\n",
    "\n",
    "        self.T, self.n_assets = self.prices.shape\n",
    "        self.feature_dim = self.features.shape[1]\n",
    "\n",
    "        self.initial_cash = float(initial_cash)\n",
    "        self.transaction_cost = float(transaction_cost)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Action: continuous allocation signals in [-1,1] for each asset\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self.n_assets,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Observation: features + position percentage per asset + cash percentage\n",
    "        self.obs_dim = self.feature_dim + self.n_assets + 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.obs_dim,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Internal state\n",
    "        self._reset_internal_state()\n",
    "\n",
    "    # ---------- internal helpers ----------\n",
    "\n",
    "    def _reset_internal_state(self):\n",
    "        \"\"\"Reset portfolio state (cash, positions, equity) and time index.\"\"\"\n",
    "        self.t = 0  # current time index\n",
    "        self.positions = np.zeros(self.n_assets, dtype=np.float32)  # number of shares per asset\n",
    "        self.cash = float(self.initial_cash)\n",
    "        self.equity = float(self.initial_cash)\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build observation vector at current time index self.t:\n",
    "        [features_t, position_value_pct, cash_pct]\n",
    "        \"\"\"\n",
    "        features_t = self.features[self.t]      # shape (F,)\n",
    "        price_t = self.prices[self.t]           # shape (N_assets,)\n",
    "\n",
    "        position_values = self.positions * price_t  # dollar value in each asset\n",
    "        total_equity = self.equity + 1e-8\n",
    "\n",
    "        position_pct = position_values / total_equity           # shape (N_assets,)\n",
    "        cash_pct = np.array([self.cash / total_equity], dtype=np.float32)\n",
    "\n",
    "        obs = np.concatenate(\n",
    "            [features_t, position_pct.astype(np.float32), cash_pct],\n",
    "            axis=0\n",
    "        )\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    # ---------- gymnasium API ----------\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        \"\"\"\n",
    "        Gymnasium reset: returns (obs, info)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self._reset_internal_state()\n",
    "        obs = self._get_observation()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One trading step:\n",
    "        - Use current prices S_t and action a_t to compute desired target positions.\n",
    "        - Apply trades, charge transaction cost.\n",
    "        - Move to t+1, compute portfolio return as reward.\n",
    "        \"\"\"\n",
    "        # Clip action into [-1,1]\n",
    "        action = np.clip(action, -1.0, 1.0).astype(np.float32)\n",
    "        price_t = self.prices[self.t]  # prices at time t\n",
    "        price_tp1 = self.prices[self.t + 1] if self.t + 1 < self.T else price_t\n",
    "\n",
    "        # 1. Compute current equity before trades\n",
    "        current_position_value = np.sum(self.positions * price_t)\n",
    "        self.equity = self.cash + current_position_value\n",
    "\n",
    "        # 2. Mapping: compute target dollar exposure and target shares\n",
    "        base_scale = self.equity / float(self.n_assets)  # E / N\n",
    "        target_values = base_scale * action               # V_i = E/N * a_i\n",
    "\n",
    "        target_positions = np.where(\n",
    "            price_t > 0,\n",
    "            target_values / price_t,\n",
    "            0.0,\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Trade size (Delta_i)\n",
    "        trade_shares = target_positions - self.positions  # + buy, - sell\n",
    "\n",
    "        # 3. Apply trades, charge transaction cost\n",
    "        trade_values = trade_shares * price_t             # signed trade notional per asset\n",
    "        dollar_turnover = np.sum(np.abs(trade_values))\n",
    "        transaction_costs = self.transaction_cost * dollar_turnover\n",
    "\n",
    "        # Update cash and positions\n",
    "        self.cash = self.cash - np.sum(trade_values) - transaction_costs\n",
    "        self.positions = self.positions + trade_shares\n",
    "\n",
    "        # 4. Move to t+1 and compute new portfolio value\n",
    "        new_position_value = np.sum(self.positions * price_tp1)\n",
    "        new_equity = self.cash + new_position_value\n",
    "\n",
    "        # Reward: simple portfolio return from t to t+1\n",
    "        reward = (new_equity - self.equity) / (self.equity + 1e-8)\n",
    "\n",
    "        # Update equity and time index\n",
    "        self.equity = new_equity\n",
    "        self.t += 1\n",
    "\n",
    "        # Episode termination when we reach the last time step\n",
    "        terminated = self.t >= self.T - 1\n",
    "        truncated = False  # could add max_steps / daily truncation if desired\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        info = {\n",
    "            \"equity\": self.equity,\n",
    "            \"cash\": self.cash,\n",
    "            \"positions\": self.positions.copy(),\n",
    "        }\n",
    "\n",
    "        # Gymnasium step output: (obs, reward, terminated, truncated, info)\n",
    "        return obs, float(reward), terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"t={self.t}, equity={self.equity:.2f}, cash={self.cash:.2f}, \"\n",
    "            f\"positions={self.positions}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PPO Agent (PyTorch)\n",
    "# ============================================================\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared base network with separate policy (mean) and value heads.\n",
    "    The policy outputs the mean of a Gaussian over actions; we later clip to [-1,1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, action_dim, hidden_sizes=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        self.base = nn.Sequential(*layers)\n",
    "\n",
    "        self.mu_head = nn.Linear(input_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        self.value_head = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base(x)\n",
    "        # keep mean in [-1,1] via tanh; actions will be sampled from Normal(mu, std)\n",
    "        mu = torch.tanh(self.mu_head(base))\n",
    "        std = torch.exp(self.log_std)\n",
    "        value = self.value_head(base).squeeze(-1)\n",
    "        return mu, std, value\n",
    "\n",
    "    def get_dist_and_value(self, obs):\n",
    "        mu, std, value = self.forward(obs)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim,\n",
    "        action_dim,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        vf_coef=0.5,\n",
    "        ent_coef=0.0,\n",
    "        max_grad_norm=0.5,\n",
    "        device=None,\n",
    "    ):\n",
    "        self.device = device or (\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.net = PolicyValueNet(obs_dim, action_dim).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Given a single observation (numpy array), return:\n",
    "        - action (numpy array)\n",
    "        - log_prob (float)\n",
    "        - value estimate (float)\n",
    "        \"\"\"\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        dist, value = self.net.get_dist_and_value(obs_t)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "        action_np = action.squeeze(0).cpu().numpy()\n",
    "        log_prob_np = log_prob.item()\n",
    "        value_np = value.item()\n",
    "        return np.clip(action_np, -1.0, 1.0), log_prob_np, value_np\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones, last_value):\n",
    "        \"\"\"\n",
    "        Generalized Advantage Estimation (GAE-Lambda)\n",
    "\n",
    "        Inputs:\n",
    "        - rewards: np.ndarray length T\n",
    "        - values: np.ndarray length T\n",
    "        - dones: np.ndarray length T (bool)\n",
    "        - last_value: scalar V(s_{T})\n",
    "\n",
    "        Outputs:\n",
    "        - advantages: np.ndarray length T\n",
    "        - returns: np.ndarray length T  (values + advantages)\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        adv = np.zeros(T, dtype=np.float32)\n",
    "        last_adv = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            mask = 1.0 - float(dones[t])\n",
    "            delta = rewards[t] + self.gamma * last_value * mask - values[t]\n",
    "            last_adv = delta + self.gamma * self.lam * mask * last_adv\n",
    "            adv[t] = last_adv\n",
    "            last_value = values[t]\n",
    "        returns = values + adv\n",
    "        return adv, returns\n",
    "\n",
    "    def update(self, batch, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        Perform PPO updates on a batch of data.\n",
    "\n",
    "        batch dict keys:\n",
    "        - 'obs': np.ndarray of shape (N, obs_dim)\n",
    "        - 'actions': np.ndarray of shape (N, action_dim)\n",
    "        - 'log_probs': np.ndarray of shape (N,)\n",
    "        - 'returns': np.ndarray of shape (N,)\n",
    "        - 'advantages': np.ndarray of shape (N,)\n",
    "        \"\"\"\n",
    "        obs = torch.as_tensor(batch[\"obs\"], dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(batch[\"actions\"], dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.as_tensor(batch[\"log_probs\"], dtype=torch.float32, device=self.device)\n",
    "        returns = torch.as_tensor(batch[\"returns\"], dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.as_tensor(batch[\"advantages\"], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        n = obs.size(0)\n",
    "        idxs = np.arange(n)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, n, batch_size):\n",
    "                end = start + batch_size\n",
    "                mb_idx = idxs[start:end]\n",
    "\n",
    "                mb_obs = obs[mb_idx]\n",
    "                mb_actions = actions[mb_idx]\n",
    "                mb_old_log_probs = old_log_probs[mb_idx]\n",
    "                mb_returns = returns[mb_idx]\n",
    "                mb_advantages = advantages[mb_idx]\n",
    "\n",
    "                dist, values = self.net.get_dist_and_value(mb_obs)\n",
    "                new_log_probs = dist.log_prob(mb_actions).sum(-1)\n",
    "                entropy = dist.entropy().sum(-1).mean()\n",
    "\n",
    "                # PPO clipped surrogate objective\n",
    "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(\n",
    "                    ratio,\n",
    "                    1.0 - self.clip_ratio,\n",
    "                    1.0 + self.clip_ratio\n",
    "                ) * mb_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Value function loss\n",
    "                value_loss = nn.functional.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Minimal training loop (gymnasium compatible)\n",
    "# ============================================================\n",
    "\n",
    "def train_ppo_on_env(\n",
    "    env: MultiAssetTradingEnv,\n",
    "    agent: PPOAgent,\n",
    "    total_steps: int = 50_000,\n",
    "    rollout_horizon: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal PPO training loop on a given environment.\n",
    "\n",
    "    This only trains in-sample. For backtesting, you will later:\n",
    "    - freeze the trained agent,\n",
    "    - run it on a separate test environment,\n",
    "    - record equity and compare against baselines.\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    step_count = 0\n",
    "\n",
    "    while step_count < total_steps:\n",
    "        # Collect one rollout\n",
    "        obs_buf = []\n",
    "        act_buf = []\n",
    "        logp_buf = []\n",
    "        rew_buf = []\n",
    "        val_buf = []\n",
    "        done_buf = []\n",
    "\n",
    "        for _ in range(rollout_horizon):\n",
    "            action, logp, value = agent.act(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            obs_buf.append(obs)\n",
    "            act_buf.append(action)\n",
    "            logp_buf.append(logp)\n",
    "            rew_buf.append(reward)\n",
    "            val_buf.append(value)\n",
    "            done_buf.append(done)\n",
    "\n",
    "            obs = next_obs\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                obs, info = env.reset()\n",
    "\n",
    "            if step_count >= total_steps:\n",
    "                break\n",
    "\n",
    "        # Compute advantage & returns using last value estimate\n",
    "        with torch.no_grad():\n",
    "            last_value = agent.net.get_dist_and_value(\n",
    "                torch.as_tensor(\n",
    "                    obs, dtype=torch.float32, device=agent.device\n",
    "                ).unsqueeze(0)\n",
    "            )[1].item()\n",
    "\n",
    "        obs_arr = np.array(obs_buf, dtype=np.float32)\n",
    "        act_arr = np.array(act_buf, dtype=np.float32)\n",
    "        logp_arr = np.array(logp_buf, dtype=np.float32)\n",
    "        rew_arr = np.array(rew_buf, dtype=np.float32)\n",
    "        val_arr = np.array(val_buf, dtype=np.float32)\n",
    "        done_arr = np.array(done_buf, dtype=bool)\n",
    "\n",
    "        adv, ret = agent.compute_gae(rew_arr, val_arr, done_arr, last_value)\n",
    "\n",
    "        batch = {\n",
    "            \"obs\": obs_arr,\n",
    "            \"actions\": act_arr,\n",
    "            \"log_probs\": logp_arr,\n",
    "            \"returns\": ret,\n",
    "            \"advantages\": adv,\n",
    "        }\n",
    "\n",
    "        agent.update(batch)\n",
    "\n",
    "        print(f\"Trained up to step {step_count}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def backtest_on_env(\n",
    "    env: MultiAssetTradingEnv,\n",
    "    agent: PPOAgent,\n",
    "    index: pd.DatetimeIndex,\n",
    "    freq_per_year: int = 252 * 390,\n",
    "    plot: bool = True,\n",
    "    save_prefix: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a frozen PPO policy on a given environment (no parameter updates),\n",
    "    record the equity curve, compute performance statistics, and optionally plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : MultiAssetTradingEnv\n",
    "        Test environment built on the out-of-sample period.\n",
    "    agent : PPOAgent\n",
    "        Trained PPO agent (parameters frozen during backtest).\n",
    "    index : pd.DatetimeIndex\n",
    "        Time index corresponding to env.prices rows (test period).\n",
    "    freq_per_year : int\n",
    "        Number of steps per trading year. For 1-minute US equity data,\n",
    "        a rough value is 252 * 390 ≈ 98,280.\n",
    "    plot : bool\n",
    "        If True, show equity and drawdown plots using matplotlib.\n",
    "    save_prefix : str or None\n",
    "        If not None, save account_value and stats to files with this prefix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_account_value : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "            - account_value: portfolio equity at each step.\n",
    "            - return: per-step return.\n",
    "    stats : dict\n",
    "        Dictionary of performance statistics (total return, Sharpe, max drawdown, etc.).\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    equities = []\n",
    "    timestamps = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _, _ = agent.act(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # env.t is the index of the NEXT state; equity is measured at that time\n",
    "        t_idx = env.t\n",
    "        if t_idx >= len(index):\n",
    "            # Safety check: avoid out-of-range\n",
    "            t_idx = len(index) - 1\n",
    "        timestamps.append(index[t_idx])\n",
    "        equities.append(info[\"equity\"])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    df_account_value = pd.DataFrame(\n",
    "        {\"date\": timestamps, \"account_value\": equities}\n",
    "    ).set_index(\"date\")\n",
    "\n",
    "    # Compute per-step returns\n",
    "    df_account_value[\"return\"] = df_account_value[\"account_value\"].pct_change().fillna(0.0)\n",
    "\n",
    "    # -------- Performance statistics --------\n",
    "    equity = df_account_value[\"account_value\"]\n",
    "    rets = df_account_value[\"return\"]\n",
    "\n",
    "    total_return = float(equity.iloc[-1] / equity.iloc[0] - 1.0)\n",
    "\n",
    "    n_steps = len(df_account_value)\n",
    "    if n_steps > 1:\n",
    "        ann_return = (1.0 + total_return) ** (freq_per_year / n_steps) - 1.0\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = float(rets.std() * np.sqrt(freq_per_year))\n",
    "    sharpe = float(ann_return / ann_vol) if ann_vol > 0 else np.nan\n",
    "\n",
    "    # Max drawdown\n",
    "    running_max = equity.cummax()\n",
    "    drawdown = equity / running_max - 1.0\n",
    "    max_dd = float(drawdown.min())\n",
    "\n",
    "    # Max drawdown duration (from last zero-dd to dd trough)\n",
    "    dd_end_time = drawdown.idxmin()\n",
    "    dd_before = drawdown.loc[:dd_end_time]\n",
    "    zero_dd = dd_before[dd_before == 0.0]\n",
    "    if not zero_dd.empty:\n",
    "        dd_start_time = zero_dd.index[-1]\n",
    "    else:\n",
    "        dd_start_time = df_account_value.index[0]\n",
    "    dd_duration = dd_end_time - dd_start_time\n",
    "\n",
    "    stats = {\n",
    "        \"start\": df_account_value.index[0],\n",
    "        \"end\": df_account_value.index[-1],\n",
    "        \"n_steps\": n_steps,\n",
    "        \"total_return\": total_return,\n",
    "        \"annual_return\": ann_return,\n",
    "        \"annual_volatility\": ann_vol,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": max_dd,\n",
    "        \"max_dd_start\": dd_start_time,\n",
    "        \"max_dd_end\": dd_end_time,\n",
    "        \"max_dd_duration\": dd_duration,\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Out-of-sample backtest statistics ===\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # -------- Visualization --------\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "        # Equity curve\n",
    "        axes[0].plot(df_account_value.index, equity, label=\"Equity\")\n",
    "        axes[0].set_ylabel(\"Equity\")\n",
    "        axes[0].set_title(\"Out-of-sample Equity Curve\")\n",
    "        axes[0].grid(True)\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Drawdown curve\n",
    "        axes[1].plot(df_account_value.index, drawdown, label=\"Drawdown\")\n",
    "        axes[1].set_ylabel(\"Drawdown\")\n",
    "        axes[1].set_xlabel(\"Time\")\n",
    "        axes[1].set_title(\"Drawdown\")\n",
    "        axes[1].grid(True)\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # -------- Optional saving --------\n",
    "    if save_prefix is not None:\n",
    "        df_account_value.to_csv(f\"{save_prefix}_account_value.csv\")\n",
    "        pd.Series(stats).to_csv(f\"{save_prefix}_stats.csv\")\n",
    "        print(f\"\\nSaved account value to {save_prefix}_account_value.csv\")\n",
    "        print(f\"Saved stats to {save_prefix}_stats.csv\")\n",
    "\n",
    "    return df_account_value, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72780670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luz02\\AppData\\Local\\Temp\\ipykernel_2924\\3941376158.py:66: DtypeWarning: Columns (8,16,24,32,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  base_df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "index is not a valid DatetimeIndex or PeriodIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2924\\3497177460.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# IMPORTANT: adjust these paths relative to where you run the script.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# If your working directory is RL_model/, then \"../data.csv\" etc. is correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# and the OHLC factors are under \"OHLC/factor_outputs\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     prices, features, idx, feat_names = build_price_and_features(\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mvol_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../volatility_forecasts.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0ma6_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../A6.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2924\\3941376158.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data_path, vol_path, a6_path, a101_path, factor_dir, tickers)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# ---------- 1. Base price data: data.csv ----------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mbase_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mbase_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateTime\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_normalize_index_utc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateTime\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mbase_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DateTime\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DateTime\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# Use Close prices as tradable prices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2924\\3941376158.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[1;34m'2025-08-04 05:30:00-04:00'\u001b[0m   \u001b[1;33m->\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m \u001b[0mUTC\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m \u001b[0mnaive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \"\"\"\n\u001b[0;32m     24\u001b[0m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# dt is now tz-aware UTC; drop tz but keep clock time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda3\\envs\\rltrading\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tz, axis, level, copy)\u001b[0m\n\u001b[0;32m  11592\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11593\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11594\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11595\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The level {level} is not valid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11596\u001b[1;33m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11599\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\rltrading\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(ax, tz)\u001b[0m\n\u001b[0;32m  11575\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_tz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11576\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz_convert\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11577\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11578\u001b[0m                     \u001b[0max_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11579\u001b[1;33m                     raise TypeError(\n\u001b[0m\u001b[0;32m  11580\u001b[0m                         \u001b[1;34mf\"{ax_name} is not a valid DatetimeIndex or PeriodIndex\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11581\u001b[0m                     )\n\u001b[0;32m  11582\u001b[0m                 \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: index is not a valid DatetimeIndex or PeriodIndex"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Example usage with REAL team data (train/test + backtest)\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Tickers must match the ones used in data.csv / signal CSVs.\n",
    "    TICKERS = (\"VOO\", \"IEMG\", \"GLDM\", \"TLT\", \"HYG\")\n",
    "\n",
    "    # IMPORTANT: adjust these paths relative to where you run the script.\n",
    "    # If your working directory is RL_model/, then \"../data.csv\" etc. is correct\n",
    "    # and the OHLC factors are under \"OHLC/factor_outputs\".\n",
    "    prices, features, idx, feat_names = build_price_and_features(\n",
    "        data_path=\"../data.csv\",\n",
    "        vol_path=\"../volatility_forecasts.csv\",\n",
    "        a6_path=\"../A6.csv\",\n",
    "        a101_path=\"../A101.csv\",\n",
    "        factor_dir=\"../OHLC/factor_outputs\",   # <- if you run from RL_model/\n",
    "        tickers=TICKERS,\n",
    "    )\n",
    "\n",
    "    print(\"Final aligned data shapes:\")\n",
    "    print(\"  prices:\", prices.shape)      # (T, 5)\n",
    "    print(\"  features:\", features.shape)  # (T, F)\n",
    "    print(\"  first timestamp:\", idx[0])\n",
    "    print(\"  last timestamp:\", idx[-1])\n",
    "\n",
    "    print(\"Any NaN in prices?\", np.isnan(prices).any())\n",
    "    print(\"Any NaN in features?\", np.isnan(features).any())\n",
    "    print(\"Any Inf in prices?\", np.isinf(prices).any())\n",
    "    print(\"Any Inf in features?\", np.isinf(features).any())\n",
    "\n",
    "\n",
    "    # 2. Train / test split (time-based)\n",
    "    T_total = prices.shape[0]\n",
    "    split_idx = int(T_total * 0.7)\n",
    "\n",
    "    prices_train = prices[:split_idx]\n",
    "    features_train = features[:split_idx]\n",
    "    idx_train = idx[:split_idx]\n",
    "\n",
    "    prices_test = prices[split_idx:]\n",
    "    features_test = features[split_idx:]\n",
    "    idx_test = idx[split_idx:]\n",
    "\n",
    "    print(\"\\nTrain / test split:\")\n",
    "    print(\"  train length:\", len(idx_train), \"from\", idx_train[0], \"to\", idx_train[-1])\n",
    "    print(\"  test length :\", len(idx_test),  \"from\", idx_test[0],  \"to\", idx_test[-1])\n",
    "\n",
    "    # 3. Build training and test environments\n",
    "    env_train = MultiAssetTradingEnv(\n",
    "        prices=prices_train,\n",
    "        features=features_train,\n",
    "        initial_cash=100000.0,\n",
    "        transaction_cost=0.0005,\n",
    "    )\n",
    "\n",
    "    env_test = MultiAssetTradingEnv(\n",
    "        prices=prices_test,\n",
    "        features=features_test,\n",
    "        initial_cash=100000.0,\n",
    "        transaction_cost=0.0005,\n",
    "    )\n",
    "\n",
    "    # 4. Initialize PPO agent (train on env_train only)\n",
    "    ppo_agent = PPOAgent(\n",
    "        obs_dim=env_train.obs_dim,\n",
    "        action_dim=env_train.n_assets,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_ratio=0.2,\n",
    "    )\n",
    "\n",
    "    # 5. In-sample training on the training window ONLY\n",
    "    train_ppo_on_env(env_train, ppo_agent, total_steps=50_000, rollout_horizon=512)\n",
    "\n",
    "    # 6. Out-of-sample backtest on the test window (frozen policy)\n",
    "    df_account_value, stats = backtest_on_env(\n",
    "        env_test,\n",
    "        ppo_agent,\n",
    "        idx_test,\n",
    "        freq_per_year=252 * 390,        # roughly 1-minute bars for US equities\n",
    "        plot=True,\n",
    "        save_prefix=\"ppo_oos\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b008ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(idx) from data.csv:\", len(idx))\n",
    "\n",
    "feat_df = pd.DataFrame(index=idx)\n",
    "feat_df = feat_df.join(vol_df, how=\"left\")\n",
    "feat_df = feat_df.join(a6_df, how=\"left\")\n",
    "feat_df = feat_df.join(a101_df, how=\"left\")\n",
    "for f in factor_frames:\n",
    "    feat_df = feat_df.join(f, how=\"left\")\n",
    "\n",
    "print(\"feat_df shape BEFORE dropna:\", feat_df.shape)\n",
    "print(\"rows with any NaN:\", feat_df.isna().any(axis=1).sum())\n",
    "print(\"rows with all NaN:\", feat_df.isna().all(axis=1).sum())\n",
    "\n",
    "feat_df = feat_df.dropna()\n",
    "print(\"feat_df shape AFTER dropna:\", feat_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rltrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
